Module,Methods,Stable,Partial,Unstable,Any,Vararg,Generic,TcFail,NoFuel,Version,Commit,Message,Date
CUDA,1511,335,77,7,944,54,92,0,2,"CUDA@3.3.1","091a4732c","Merge pull request #1022 from JuliaGPU/tb/llvm","2021-07-09 07:39:58 +0200"
CUDA,1511,335,77,7,944,54,92,2,0,"CUDA@3.3.1","43e0f48b1","Merge pull request #1048 from JuliaGPU/update_manifest","2021-07-15 11:13:23 +0200"
CUDA,1511,335,77,7,944,54,92,0,2,"CUDA@3.3.2","c73db14fc","Bump patch release","2021-07-15 11:14:58 +0200"
CUDA,1511,335,77,7,944,54,92,2,0,"CUDA@3.3.2","d1c02b749","Use correct intrinsics for rem (#1041)","2021-07-18 22:41:03 -0700"
CUDA,1511,335,77,7,944,54,92,0,2,"CUDA@3.3.2","74110682d","Remove broken workflow.","2021-07-19 08:15:30 +0200"
CUDA,1504,339,67,7,934,54,101,2,0,"CUDA@3.4.0","76a409107","Merge pull request #1059 from JuliaGPU/tb/atomics","2021-07-22 07:39:24 +0200"
CUDA,1504,340,68,5,934,54,101,0,2,"CUDA@3.4.0","b934949a1","Return the device from device!.","2021-07-22 08:23:42 +0200"
CUDA,1513,346,68,5,935,54,103,0,2,"CUDA@3.4.0","0f8e2a174","Merge pull request #1060 from JuliaGPU/tb/version_vars","2021-07-22 15:33:57 +0200"
CUDA,1514,347,68,5,935,54,103,0,2,"CUDA@3.4.0","eb240c0e6","Merge pull request #1030 from JuliaGPU/tb/find_librar","2021-07-22 17:13:12 +0200"
CUDA,1526,350,72,5,938,54,105,0,2,"CUDA@3.4.0","db639be9b","Merge pull request #1023 from JuliaGPU/tb/unified","2021-07-29 08:09:39 +0200"
CUDA,1527,351,72,5,938,54,105,0,2,"CUDA@3.4.0","b22abc3f3","Merge pull request #1068 from JuliaGPU/tb/async","2021-07-29 17:48:14 +0200"
CUDA,1525,349,72,5,938,54,105,1,1,"CUDA@3.4.0","57726995a","Merge pull request #1069 from JuliaGPU/tb/latency2","2021-07-30 07:46:27 +0200"
CUDA,1525,349,72,5,938,54,105,0,2,"CUDA@3.4.0","2765fe3d8","Merge pull request #1083 from JuliaGPU/fp/conversions","2021-08-05 16:27:16 +0200"
CUDA,1525,350,71,5,938,54,105,0,2,"CUDA@3.4.0","5716b0efa","Merge pull request #1082 from JuliaGPU/tb/randn","2021-08-05 17:32:14 +0200"
CUDA,1526,351,71,5,938,54,105,0,2,"CUDA@3.4.0","eaad8b032","Merge pull request #1103 from JuliaGPU/tb/hotfix","2021-08-17 13:43:24 +0200"
CUDA,1527,351,71,5,939,54,105,0,2,"CUDA@3.4.1","9b2d5089c","Introduce a macro for marking multiple functions as device-only. (#1117)","2021-08-26 12:01:15 +0200"
CUDA,1531,352,71,5,939,54,108,0,2,"CUDA@3.4.1","afe817940","Merge pull request #1114 from JuliaGPU/tb/shmem","2021-08-26 13:58:25 +0200"
CUDA,1531,352,71,5,939,54,108,1,1,"CUDA@3.4.1","72de2652e","Simplify library loading. (#1121)","2021-08-27 14:48:14 +0200"
CUDA,1533,354,71,5,939,54,108,1,1,"CUDA@3.4.1","6e057e771","Support wrapping a host buffer with a CuArray (#1131)","2021-09-07 13:56:35 +0200"
CUDA,1534,353,71,7,939,54,108,1,1,"CUDA@3.4.1","5cbefc628","Use the kernel state object to pass the exception flag location.","2021-08-23 09:11:41 +0200"
CUDA,1536,354,71,7,940,54,108,1,1,"CUDA@3.4.1","cd1d63cce","Merge pull request #1147 from JuliaGPU/tb/native_device_synchronize","2021-09-17 22:14:26 +0200"
CUDA,1535,355,71,6,939,54,108,1,1,"CUDA@3.4.1","6dbcedcfc","Merge pull request #1157 from JuliaGPU/tb/sync","2021-09-20 13:21:04 +0200"
CUDA,1546,357,78,8,939,54,108,1,1,"CUDA@3.4.1","b553d4967","Streamline the random number interface. (#1146)","2021-09-22 17:27:42 +0200"
CUDA,1547,358,78,8,939,54,108,1,1,"CUDA@3.4.1","45f65b3bf","Merge pull request #1159 from JuliaGPU/tb/bindeps","2021-09-23 12:40:56 +0200"
CUDA,1548,360,78,6,940,54,108,1,1,"CUDA@3.4.1","fbfdcc2d0","Pass kernel state by value.","2021-09-29 18:37:43 +0200"
CUDA,1557,369,79,6,939,54,108,1,1,"CUDA@3.4.1","3b885462f","Merge pull request #1135 from JuliaGPU/tb/uniform_getters","2021-10-01 10:42:18 +0200"
CUDA,1563,369,82,6,940,54,110,1,1,"CUDA@3.4.1","e0c63a29f","Support reinterpret(reshape). (#1149)","2021-10-01 04:43:30 -0400"
CUDA,1561,367,82,6,940,54,110,1,1,"CUDA@3.4.1","2686c9e0d","Merge pull request #1177 from JuliaGPU/tb/pool_reclaim","2021-10-02 14:49:23 +0200"
CUDA,1562,367,83,6,940,54,110,1,1,"CUDA@3.4.1","d175c2933","Merge pull request #1176 from JuliaGPU/tb/context_switch","2021-10-02 14:49:35 +0200"
CUDA,1561,366,83,6,940,54,110,1,1,"CUDA@3.4.1","de5827ff7","Update documentation (#1187)","2021-10-05 15:03:59 +0200"
CUDA,1562,367,83,6,940,54,110,1,1,"CUDA@3.4.1","b3f98c6f9","Fix CI.","2021-10-06 17:39:08 +0200"
CUDA,1565,368,83,6,942,54,110,2,0,"CUDA@3.5.0","0fafe91e4","Correctly handle multi-GPU instances with NVML. (#1199)","2021-10-14 13:54:09 +0200"
CUDA,1570,373,83,6,942,54,110,2,0,"CUDA@3.5.0","211cfb19e","Merge pull request #1207 from JuliaGPU/tb/improvements","2021-10-18 21:20:10 +0200"
CUDA,1574,376,83,6,943,54,110,2,0,"CUDA@3.5.0","80a5978f2","Deprecate non-blocking sync, and always call the API. (#1213)","2021-10-22 07:30:28 +0200"
CUDA,1591,386,86,6,945,54,112,2,0,"CUDA@3.5.0","103b958bf","Fix type of LLVM value.","2021-11-30 16:45:35 +0100"
CUDA,1590,385,86,6,945,54,112,2,0,"CUDA@3.5.0","3d7c15eef","Use Base functionality for iteration Union type components. (#1257)","2021-12-01 07:43:00 +0100"
CUDA,1590,385,86,6,945,54,112,1,1,"CUDA@3.5.0","4f6dfbdec","Bump CI to Julia 1.7. (#1260)","2021-12-09 13:00:36 +0100"
CUDA,1590,385,86,6,945,54,112,2,0,"CUDA@3.5.0","6a2a2967b","Use CUDA APIs for unoptimized copies. (#1265)","2021-12-10 07:52:43 +0100"
CUDA,1590,385,86,6,945,54,112,1,1,"CUDA@3.6.3","273781b1d","Bump version.","2022-01-06 13:39:38 +0100"
CUDA,1590,385,86,6,945,54,112,2,0,"CUDA@3.6.3","0d4ad5d72","Merge pull request #1310 from JuliaGPU/tb/artifacts","2022-01-08 14:34:44 +0100"
CUDA,1591,386,86,6,945,54,112,2,0,"CUDA@3.7.0","b63bc1c22","Merge pull request #1333 from JuliaGPU/tb/libcuda_fail","2022-01-24 15:04:15 +0100"
CUDA,1592,386,86,6,946,54,112,2,0,"CUDA@3.7.0","baa8d787c","Add `CUDA.return_type` (#1339)","2022-01-26 00:13:40 -0800"
CUDA,1589,386,86,6,944,53,112,2,0,"CUDA@3.7.0","64c5ca8f9","Merge pull request #1338 from JuliaGPU/tb/cudadrv_state","2022-01-26 11:44:36 +0100"
CUDA,1594,389,86,6,946,53,112,2,0,"CUDA@3.7.0","ca77d1828","Merge pull request #1340 from JuliaGPU/tb/compute_sanitizer","2022-01-26 13:27:56 +0100"
CUDA,1600,394,86,6,947,53,112,2,0,"CUDA@3.7.0","0c9d886bd","Merge pull request #1284 from JuliaGPU/ksh/peer","2022-01-26 16:01:51 +0100"
CUDA,1602,394,86,6,947,53,114,2,0,"CUDA@3.7.0","430787db7","Don't assume host pointers are directly usable on the device. (#1342)","2022-01-26 18:44:51 +0100"
CUDA,1603,395,86,6,947,53,114,1,1,"CUDA@3.7.0","eb332f7ed","Merge pull request #1344 from JuliaGPU/tb/pool","2022-01-28 16:56:00 +0100"
CUDA,1603,395,86,6,947,53,114,2,0,"CUDA@3.8.0","a903a6e24","Merge pull request #1345 from JuliaGPU/tb/memoize","2022-01-28 22:46:36 +0100"
CUDA,1603,395,86,6,947,53,114,1,1,"CUDA@3.8.0","f7a9e4158","Merge pull request #1356 from JuliaGPU/tb/fastmath","2022-02-07 17:33:10 +0100"
CUDA,1604,396,86,6,947,53,114,2,0,"CUDA@3.8.0","b7764ab74","Merge pull request #1357 from JuliaGPU/tb/pool_peer_access","2022-02-08 13:57:01 +0100"
CUDA,1610,396,86,6,953,53,114,2,0,"CUDA@3.8.0","18738cf22","Merge pull request #1391 from JuliaGPU/tb/no_toplevel_compilation","2022-02-22 11:22:56 +0100"
CUDA,1610,396,86,6,953,53,114,1,1,"CUDA@3.8.0","c616fe853","Fix and test math precision. (#1394)","2022-02-23 12:11:30 +0100"
CUDA,1610,396,86,6,953,53,114,2,0,"CUDA@3.8.0","950f54fae","Merge pull request #1397 from JuliaGPU/tb/artifacts","2022-02-24 08:42:14 +0100"
CUDA,1610,396,86,6,953,53,114,1,1,"CUDA@3.8.0","764a5b1e2","Merge pull request #1404 from JuliaGPU/tb/bindeps_errors","2022-02-24 18:15:56 +0100"
CUDA,1612,397,86,6,953,53,115,1,1,"CUDA@3.8.0","95eec2173","Merge pull request #1400 from bjarthur/bja/atomicbfloat16","2022-02-24 18:40:33 +0100"
CUDA,1613,398,86,6,953,53,115,1,1,"CUDA@3.8.0","12da74b30","Report the OOM memory status at the time of the error. (#1428)","2022-03-03 16:00:29 +0100"
CUDA,1614,398,87,6,953,53,115,1,1,"CUDA@3.8.0","86b5069ab","Switch to new LLVM context management for 1.9 compatibility. (#1432)","2022-03-08 11:55:46 +0100"
CUDA,1614,398,87,6,953,53,115,2,0,"CUDA@3.8.0","0bd5b39b5","Add NCCL binaries. (#1450)","2022-03-22 07:59:18 +0100"
CUDA,1563,395,82,6,922,53,103,1,1,"CUDA@3.11.0","003d14369","Import internal factorisation types from LinearAlgebra (#1558)","2022-07-06 19:27:33 +0530"
CUDA,1564,395,82,6,922,53,104,1,1,"CUDA@3.11.0","daa158065","Add reshape for CuDeviceArray (#1561)","2022-07-11 14:45:22 +0200"
CUDA,1564,395,82,6,922,53,104,0,2,"CUDA@3.12.0","b8b30a851","Use Base.active_project. (#1576)","2022-08-09 14:10:26 +0200"
CUDA,1564,395,82,6,922,53,104,1,1,"CUDA@3.12.0","63e07c02c","Update dependencies. (#1578)","2022-08-16 08:55:16 +0200"
CUDA,1563,395,81,6,922,52,105,1,1,"CUDA@3.12.0","dbc767b1b","Fixes for and tests using JET. (#1577)","2022-08-17 13:28:17 +0200"
CUDA,1563,395,82,6,924,52,102,1,1,"CUDA@3.12.0","f5e3e9c26","remove unbound type parameters (#1585)","2022-09-08 21:57:52 +0200"
CUDA,1564,395,82,6,925,52,102,1,1,"CUDA@3.12.0","c69911305","Provide more useful explanation why an eltype is unsupported. (#1596)","2022-09-20 12:24:25 +0200"
CUDA,1568,395,82,6,929,52,102,1,1,"CUDA@3.12.0","603edb878","Improve eltype error reporting. (#1598)","2022-09-20 21:33:02 +0200"
CUDA,1562,389,82,6,929,52,102,1,1,"CUDA@3.12.0","8a4cbdee5","Move CUDNN and CUTENSOR into separate packages (#1624)","2022-10-12 15:00:20 +0200"
CUDA,1569,391,82,6,934,52,102,0,2,"CUDA@4.0.0","014fcef47","Merge pull request #1629 from JuliaGPU/tb/jll","2022-10-21 20:41:19 +0200"
CUDA,1649,391,82,6,1014,52,102,1,1,"CUDA@4.0.0","f9045bdf1","Update autogenerated headers (#1649)","2022-10-26 13:21:03 +0200"
CUDA,1640,384,82,6,1012,52,102,2,0,"CUDA@4.0.0","98ede6e3f","Remove deprecations (#1651)","2022-10-26 18:29:28 +0200"
CUDA,1641,384,82,6,1013,52,102,2,0,"CUDA@4.0.0","8f0850da1","Avoid scalar indexing with accumulate of ND-input without dims (#1681)","2022-12-01 12:34:35 +0100"
CUDA,1644,385,84,6,1013,52,102,2,0,"CUDA@4.0.0","f9822e3af","Re-introduce memory limits. (#1698)","2022-12-21 12:40:48 +0100"
CUDA,1646,385,84,6,1013,52,104,2,0,"CUDA@4.0.0","024534346","Remove/rework CuDeviceArray constructors (#1308)","2022-12-21 21:55:32 +0100"
CUDA,1642,385,84,6,1009,52,104,2,0,"CUDA@4.0.0","c35bc08e1","Use released dependencies. (#1732)","2023-01-18 22:23:32 +0100"
CUDA,1641,384,84,6,1009,52,104,2,0,"CUDA@4.0.0","72b0796f8","Introduce cuFFT plan cache; switch to auto-managed memory. (#1734)","2023-01-20 13:57:53 +0100"
CUDA,1672,384,84,6,1040,52,104,2,0,"CUDA@4.0.1","aca936bc5","Add support for CUDA 12.0. (#1742)","2023-03-11 12:58:36 +0100"
CUDA,1694,384,84,6,1062,52,104,1,1,"CUDA@4.0.1","e9141d5ab","Add support for CUDA 12.1 (#1793)","2023-03-12 17:30:21 +0100"
CUDA,1695,383,83,6,1065,52,104,1,1,"CUDA@4.0.1","a218b9cfb","Merge pull request #1799 from JuliaGPU/tb/world_ages","2023-03-15 11:31:00 +0100"
CUDA,1696,385,83,6,1064,52,104,2,0,"CUDA@4.1.3","5c51766d0","Protect against calling set_runtime_version with invalid contents.","2023-04-04 16:50:02 +0200"
